"""
Load the existing vector store and perform custom searches with AI-powered answers
"""

import sys
import os
from dotenv import load_dotenv

# Load environment variables from .env file
load_dotenv()

# Fix Windows console encoding
if sys.platform == 'win32':
    sys.stdout.reconfigure(encoding='utf-8')

from langchain_chroma import Chroma
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnablePassthrough

print("=" * 60)
print("AIå°åŠ©æ‰‹ - çŸ¥è¯†åº“æ£€ç´¢")
print("=" * 60)
print("\nLoading vector store...")

# Initialize the same embeddings model used during indexing
# ä½¿ç”¨ä¸­æ–‡åµŒå…¥æ¨¡å‹ - å¿…é¡»ä¸vector_store_retrieval.pyä¸­çš„æ¨¡å‹ä¸€è‡´
embeddings = HuggingFaceEmbeddings(
    model_name="BAAI/bge-base-zh-v1.5",
    model_kwargs={'device': 'cpu'},
    encode_kwargs={'normalize_embeddings': True}
)

# Load the existing vector store from persistent directory
persist_directory = "./chroma_db_VN"
try:
    vector_store = Chroma(
        collection_name="VN_knowledgeBase",
        embedding_function=embeddings,
        persist_directory=persist_directory
    )
    
    # Check if the vector store has documents
    collection = vector_store._collection
    count = collection.count()
    
    if count == 0:
        print("âŒ Vector store is empty!")
        print("\nPlease run 'python vector_store_retrieval.py' first to create and populate the vector store.")
        sys.exit(1)
    
    print(f"âœ“ Vector store loaded successfully! ({count} document chunks)")
except Exception as e:
    print(f"âŒ Error loading vector store: {e}")
    print("\nPlease run 'python vector_store_retrieval.py' first to create the vector store.")
    sys.exit(1)

# Create a retriever
retriever = vector_store.as_retriever(
    search_type="similarity",
    search_kwargs={"k": 2}  # å¢åŠ æ£€ç´¢æ•°é‡ä»¥è·å¾—æ›´å¤šç›¸å…³ä¸Šä¸‹æ–‡
)

# ============================================
# SET UP RAG WITH OPENAI
# ============================================
use_rag = False
llm = None

if os.environ.get("OPENAI_API_KEY"):
    try:
        from langchain_openai import ChatOpenAI
        
        llm = ChatOpenAI(model="gpt-3.5-turbo", temperature=0)
        
        template = """æ‚¨æ˜¯ä¸€å AI åŠ©æ‰‹ï¼Œå¸®åŠ©å›ç­”æœ‰å…³VNé¡¹ç›®çš„çŸ¥è¯†é—®é¢˜ã€‚è¯·ä½¿ç”¨ä»æ–‡æ¡£ä¸­æ£€ç´¢åˆ°çš„ä»¥ä¸‹ä¸Šä¸‹æ–‡æ¥å›ç­”é—®é¢˜ã€‚
        å¦‚æœæ‚¨æ— æ³•æ ¹æ®ä¸Šä¸‹æ–‡å›ç­”é—®é¢˜ï¼Œè¯·è¯´â€œæˆ‘æ— æ³•åœ¨æä¾›çš„ä¸Šä¸‹æ–‡ä¸­æ‰¾åˆ°è¯¥ä¿¡æ¯ã€‚â€è¯·ç®€æ˜è€Œå…·ä½“åœ°å›ç­”.

Context:
{context}

Question: {question}

Answer:"""
        
        prompt = ChatPromptTemplate.from_template(template)
        
        # Helper function to format documents
        def format_docs(docs):
            return "\n\n".join(f"[Page {doc.metadata.get('page', 'N/A')}]\n{doc.page_content}" for doc in docs)
        
        # Create RAG chain
        rag_chain = (
            {"context": retriever | format_docs, "question": RunnablePassthrough()}
            | prompt
            | llm
            | StrOutputParser()
        )
        
        use_rag = True
        print("\nâœ… OpenAI RAG mode enabled! AI will answer your questions using retrieved context.")
    except Exception as e:
        print(f"\nâš ï¸  OpenAI setup failed: {e}")
        print("Falling back to search-only mode.")
else:
    print("\nğŸ’¡ Tip: Set OPENAI_API_KEY environment variable to enable AI-powered answers!")
    print("   For now, showing retrieved passages only.")

print("\n" + "=" * 60)
if use_rag:
    print("ğŸš€ Ready! Ask questions - AI will answer using VN project knowledge base.")
else:
    print("Ready! Ask questions about VN project knowledge base.")
print("Type 'quit' or 'exit' to stop.")
print("=" * 60)

print("\nğŸ’¡ Example questions:")
print("  - æ•´ä¸ªé¡¹ç›®çš„ç³»ç»Ÿç»“æ„æ˜¯æ€æ ·çš„?")
print("  - è½¦è¾†çš„å¤–è§‚æ£€æŸ¥å†…å®¹æœ‰å“ªäº›?")
print("  - åœ¨ç»™AGVå»ºå›¾çš„æ—¶å€™æœ‰å“ªäº›æ³¨æ„äº‹é¡¹?")


while True:
    try:
        print("\n" + "-" * 60)
        user_query = input("\nâ“ Your question: ").strip()
        
        if user_query.lower() in ['quit', 'exit', 'q']:
            print("\nGoodbye! ğŸ‘‹")
            break
        
        if not user_query:
            continue
        
        print(f"\nğŸ” Searching for: '{user_query}'")
        
        # Get relevant documents
        results = retriever.invoke(user_query)
        
        # If RAG is enabled, get AI answer
        if use_rag:
            print("\nğŸ¤– AI Answer:")
            print("=" * 60)
            try:
                answer = rag_chain.invoke(user_query)
                print(answer)
            except Exception as e:
                print(f"Error generating answer: {e}")
            print("=" * 60)
        
        # Show retrieved passages
        print("\nğŸ“„ Retrieved Passages:\n")
        for i, doc in enumerate(results, 1):
            print(f"{i}. Page {doc.metadata.get('page', 'N/A')}")
            print("-" * 40)
            # Show first 400 chars for readability
            content = doc.page_content[:400] + ("..." if len(doc.page_content) > 400 else "")
            print(content)
            print()
        
        # Show similarity scores
        results_with_scores = vector_store.similarity_search_with_score(user_query, k=3)
        print("ğŸ“Š Similarity Scores (lower is better):")
        for i, (doc, score) in enumerate(results_with_scores, 1):
            print(f"  {i}. Page {doc.metadata.get('page', 'N/A')}: {score:.4f}")
        
    except KeyboardInterrupt:
        print("\n\nGoodbye! ğŸ‘‹")
        break   
    except Exception as e:
        print(f"\nâŒ Error: {e}")

